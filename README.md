# rusted_graw

rusted_graw is a Rust event builder and data merger inspired by [graw-merger](https://github.com/ATTPC/graw-merger) used to combine and convert `.graw` files generated by the AT-TPC data acquisition to the more universal and flexible [HDF5](https://www.hdfgroup.org/) format.

## Installation

### Rust

If you have not used Rust before, you will most likely need to install the Rust tool chain. See the [Rust docs](https://www.rust-lang.org/tools/install) for installation instructions.

### Platform Support

rusted_graw aims to support Linux, MacOS, and Windows. Currently, rusted_graw has been tested and built successfully on Ubuntu 22.04 and MacOS 13 (Ventura).

### Downloading

To download rusted_graw clone the git repository using `git clone https://github.com/gwm17/rusted_graw.git`

### HDF5

Before building and running rusted_graw, HDF5 must be installed. Typically this will be installed using a package manager (homebrew, apt, etc), and the Rust libraries will auto detect the location of the HDF install. However, this is not always possible. Sometimes a newer version will need to be installed to a custom location. If this is the case, write the following snippet into the file `.cargo/config.toml` in the rusted_graw repository:

```[toml]
[env]
HDF5_DIR="/path/to/my/hdf5/install/"

[build]
rustflags="-C link-args=-Wl,-rpath,/path/to/my/hdf5/install/lib"
```

Replace `/path/to/my/hdf5/install/` with the path to your HDF5 installation. The extra build command assumes that the hdf5 files are not installed to the normal library search path of your operating sytsem. Note that you will need to create the `.cargo` directory and the `config.toml` file.

### Building

To build rusted_graw simply run `cargo build --release` from the rusted_graw repository to build the optimized release binary. To build debug version use `cargo build`. Note that if build errors are present and reference HDF5, your HDF5 install may be in a non-standard location. See the above section to specify custom HDF5 install locations to the build system.

### Running

To run rusted_graw use `cargo run --release` from the rusted_graw repository. This will spawn the rusted_graw UI. Additionally, rusted_graw will log information to the terminal.

## Configuration

The rusted_graw UI has 5 input fields that the user needs to fill out to process a run:

- GRAW directory: Specifies the full-path to a directory which contains the AT-TPC graw structure (i.e. contains subdirectories of the run_# format)
- EVT directory: Specifies the full-path to a directory which contains the FRIBDAQ evt structure (i.e. contains subdirectories of the run# format)
- HDF5 directory: Specifies the full-path to a directory to which hdf5 (.h5) files will be written
- Pad map: Specifies the full path to a CSV file which contains the mapping information for AT-TPC pads and electronics
- Run Number: Which run should be processed

The configuration can be saved (to a .yaml format) using File -> Save...
Configuration files can be loaded using File -> Open...
Using the Open buttons next to the directory/file fields will bring up a file dialog for those elements

### Online

The rusted_graw UI additionally contains a button for toggling online mode. When toggled on, this changes the
interpretation of the various paths to match the behavior of the AT-TPC DAQ network.

## Output

rusted_graw will output two files: the final resulting HDF5 data file, and a log file. Log files contain valuable information about the status of the application while building the merged data. If an error occurs, typically a warning will be printed to the terminal indicating that the user should check the log file. The log file will contain the detailed status of the run and indicate the issue that occurred. Log files are also useful because they can be easily shared when errors occur. It is not advised to delete the log files.

### HDF5 Data Format

The data format used in the HDF5 data is as follows:

- All FRIBDAQ data is within the Group "frib"
- FRIBDAQ Physics items are stored in the "evt" group
- FRIBDAQ Scaler items are stored in the "scaler" group
- All GET data is within the Group named "get"
- Each event has two Datasets. One is "evt#_data" and one is "evt#_header". The Datasets are named by event number (i.e. event 101 corresponds to Dataset evt101_data).
- The "header" Datasets contain metadata about the event (number and timestamp).
- Each "data" Dataset contains a two dimensional matrix of traces. Each row contains the data for a single trace from a pad in AT-TPC. The first five elements of the row contain the electronic address of the the pad (CoBo, AsAd, AGET, Channel, Pad in that order); the remaining 512 elements contain the trace data.
- Traces are stored in random order. That is, the Dataset matrix rows are not sorted by electronic address.

Meta data can be found in many places. This will be cleaned up in future versions.
